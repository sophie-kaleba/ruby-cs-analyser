# Structure
- truffleruby (submodule)
Holds an instrumented version of TruffleRuby. Can output an execution trace that track most of method and block calls.
- graal (submodule)
Holds a variation of GraalVM that uses a unique ID to identify call targets
- behaviour-analysis (submodule)
Holds the scripts necessary to analyse execution traces and output an analysis report.
Also output summary tables usable in tex files
- results
Holds the execution traces and the analysis results

# How to use
The Makefile contains all the necessary targets to run ruby programs and analyse execution traces

    make all benchmark_name="---" iterations="---" inner_iterations="---"

should do the trick. As this will trigger the submodule fetch and the building of truffleruby, I'd rather recommand to call

    make init

first, and then later only run

    make do_run do_analyse do_report benchmark_name="---" iterations="---" inner_iterations="---"

each time a new application needs to be analysed.

### Makefile targets

**fetch_deps:** fetch the submodules, update them and checkout the correct branches

**build_tr:** build truffleruby (jvm-ce) and compile the splitting analyser 
     
**run_and_log:** run the ruby application.
The execution trace is stored in ./results/[date]/[app_name]/raw_[app_name].log
Coverage data is stored in ./results/[date]/[app_name]/Coverage

**parse_coverage:** summarise and parse the coverage files so they can later be turned into latex tables

**parse_trace:** clean and format the execution trace file

**analyse_trace:** analyse the execution trace and compute summary statistics about polymorphism and splitting. Summaries are stored in ./results/[date]/[app_name]/[Methods|Blocks]
	  
**report:** generate a latex report, available at ./results/[date]/[app_name]/report

**plots:** generate two kinds of plots, for the 30th hottest call-sites.

**clean:** clean auxiliary files generated by the report target


# TODO
- [X] Analyse startup data
- [ ] Generate report for blocks
- [ ] list packages dependency / run on a fresh VM
- [ ] Make it possible to run a list of programs, analyse all the traces and aggregate all the results
- [x] Make sure the input for the java splitting analyzer is written in the currnent benchmark folder
- [x] Get rid of the dependency to the harness: make it log from the start...
- [x] ...with the possibility to: - identify if startup - disable logging during startup
- [ ] R performance: str_trim when python parsing rather than in the R script
- [x] Analysis takes time, and most of it is spent on the java part of the process. Improve the java splitting analyser.
- [x] Generate the execution plots and store them in the benchmark folder
- [x] Clean the behaviour-analysis repository
- [x] Generate a target lifetime plot (y-axis is target ID), color coded depending on the receiver set
